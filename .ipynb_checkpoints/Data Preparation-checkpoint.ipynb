{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "This guide was written in Python 3.6.\n",
    "\n",
    "### Python and Pip\n",
    "\n",
    "Download [Python](https://www.python.org/downloads/) and [Pip](https://pip.pypa.io/en/stable/installing/).\n",
    "\n",
    "### Other\n",
    "\n",
    "Let's install the modules we'll need for this tutorial. Open up your terminal and enter the following commands to install the needed python modules: \n",
    "\n",
    "```\n",
    "pip3 install os\n",
    "pip3 install pandas\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "We've gone over Data Acquisition as of now, so we know how to <i>get</i> our data. But once you have the data, it might not be in the best shape. You might have scraped a bunch of data from a website, but need it in the form of a dataframe to work with it in an easier manner. This process is called data preparation - preparing your data in a format that's easiest to form with.\n",
    "\n",
    "### Overview\n",
    "\n",
    "<b> Data Acquisition: </b> Reading and writing with a variety of file formats and databases. <br>\n",
    "<b> Preparation: </b> Cleaning, munging, combining, normalizing, reshaping, slicing and dicing, and transforming data for analysis. <br>\n",
    "<b> Transformation: </b> Applying mathematical and statistical operations to groups of data sets to derive new data sets. For example, aggregating a large table by group variables. <br>\n",
    "<b> Modeling and computation: </b> Connecting your data to statistical models, machine learning algorithms, or other computational tools <br>\n",
    "<b> Presentation: </b> Creating interactive or static graphical visualizations or textual summaries <br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pandas\n",
    "\n",
    "Pandas allows us to deal with data in a way that us humans can understand it - with labelled columns and indexes. It allows us to effortlessly import data from files such as CSVs, allows us to quickly apply complex transformations and filters to our data and much more. Along with Numpy and Matplotlib, it helps create a really strong base for data exploration and analysis in Python. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas import Series, DataFrame\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Series\n",
    "\n",
    "A Series is a one-dimensional array-like object containing an array of data (of any NumPy data type) and an associated array of data labels, called its index. The simplest Series is formed from only an array of data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    4\n",
      "1    7\n",
      "2   -5\n",
      "3    3\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "obj = Series([4, 7, -5, 3])\n",
    "list1 = [4,7,-5,3]\n",
    "print(Series(list1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Often it will be desirable to create a Series with an index identifying each data point:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d    4\n",
      "b    7\n",
      "a   -5\n",
      "c    3\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "obj2 = Series([4, 7, -5, 3], index=['d', 'b', 'a', 'c'])\n",
    "print(obj2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also take a dictionary and convert it to a Series:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ohio      35000\n",
      "Texas     71000\n",
      "Oregon    16000\n",
      "Utah       5000\n",
      "dtype: int64\n",
      "<class 'pandas.core.series.Series'>\n"
     ]
    }
   ],
   "source": [
    "sdata = {'Ohio': 35000, 'Texas': 71000, 'Oregon': 16000, 'Utah': 5000}\n",
    "obj3 = Series(sdata)\n",
    "print(obj3)\n",
    "print(type(obj3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataFrames\n",
    "\n",
    "A DataFrame represents a tabular, spreadsheet-like data structure containing an ordered collection of columns, each of which can be a different value type (numeric, string, boolean, etc.).\n",
    "\n",
    "There are numerous ways to construct a DataFrame, though one of the most common is from a dict of equal-length lists or NumPy arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {'state': ['Ohio', 'Ohio', 'Ohio', 'Nevada', 'Nevada'], \n",
    "        'year': [2000, 2001, 2002, 2001, 2002], \n",
    "        'pop': [1.5, 1.7, 3.6, 2.4, 2.9]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we take this and convert it to a DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame = DataFrame(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gets us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    state  year  pop\n",
      "0    Ohio  2000  1.5\n",
      "1    Ohio  2001  1.7\n",
      "2    Ohio  2002  3.6\n",
      "3  Nevada  2001  2.4\n",
      "4  Nevada  2002  2.9\n"
     ]
    }
   ],
   "source": [
    "print(frame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also specify the sequence of columns by:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>state</th>\n",
       "      <th>pop</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2000</td>\n",
       "      <td>Ohio</td>\n",
       "      <td>1.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2001</td>\n",
       "      <td>Ohio</td>\n",
       "      <td>1.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2002</td>\n",
       "      <td>Ohio</td>\n",
       "      <td>3.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2001</td>\n",
       "      <td>Nevada</td>\n",
       "      <td>2.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2002</td>\n",
       "      <td>Nevada</td>\n",
       "      <td>2.9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   year   state  pop\n",
       "0  2000    Ohio  1.5\n",
       "1  2001    Ohio  1.7\n",
       "2  2002    Ohio  3.6\n",
       "3  2001  Nevada  2.4\n",
       "4  2002  Nevada  2.9"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DataFrame(data, columns=['year', 'state', 'pop'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply\n",
    "\n",
    "Lets's generate a random dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               b         d         e\n",
      "Utah    0.286915  0.756919 -1.151341\n",
      "Ohio   -0.517600  0.127705 -0.273800\n",
      "Texas  -1.314092  0.307776  1.301489\n",
      "Oregon -1.855136 -1.452916  0.087474\n"
     ]
    }
   ],
   "source": [
    "frame = DataFrame(np.random.randn(4, 3), columns=list('bde'), index=['Utah', 'Ohio', 'Texas', 'Oregon'])\n",
    "\n",
    "print(frame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this, we can apply a function on a DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>b</th>\n",
       "      <th>d</th>\n",
       "      <th>e</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Utah</th>\n",
       "      <td>0.286915</td>\n",
       "      <td>0.756919</td>\n",
       "      <td>1.151341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ohio</th>\n",
       "      <td>0.517600</td>\n",
       "      <td>0.127705</td>\n",
       "      <td>0.273800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Texas</th>\n",
       "      <td>1.314092</td>\n",
       "      <td>0.307776</td>\n",
       "      <td>1.301489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Oregon</th>\n",
       "      <td>1.855136</td>\n",
       "      <td>1.452916</td>\n",
       "      <td>0.087474</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               b         d         e\n",
       "Utah    0.286915  0.756919  1.151341\n",
       "Ohio    0.517600  0.127705  0.273800\n",
       "Texas   1.314092  0.307776  1.301489\n",
       "Oregon  1.855136  1.452916  0.087474"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.abs(frame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also apply functions with the `apply()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b    2.142052\n",
      "d    2.209835\n",
      "e    2.452830\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "f = lambda x: x.max() - x.min()\n",
    "\n",
    "print(frame.apply(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               b         d         e\n",
      "Utah    0.286915  0.756919  1.151341\n",
      "Ohio    0.517600  0.127705  0.273800\n",
      "Texas   1.314092  0.307776  1.301489\n",
      "Oregon  1.855136  1.452916  0.087474\n"
     ]
    }
   ],
   "source": [
    "f = lambda x: np.abs(x)\n",
    "\n",
    "frame = frame.apply(f)\n",
    "\n",
    "print(frame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sorting\n",
    "\n",
    "To sort lexicographically by row or column index, use the sort_index method, which returns a new, sorted object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>b</th>\n",
       "      <th>d</th>\n",
       "      <th>e</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Ohio</th>\n",
       "      <td>0.517600</td>\n",
       "      <td>0.127705</td>\n",
       "      <td>0.273800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Oregon</th>\n",
       "      <td>1.855136</td>\n",
       "      <td>1.452916</td>\n",
       "      <td>0.087474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Texas</th>\n",
       "      <td>1.314092</td>\n",
       "      <td>0.307776</td>\n",
       "      <td>1.301489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Utah</th>\n",
       "      <td>0.286915</td>\n",
       "      <td>0.756919</td>\n",
       "      <td>1.151341</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               b         d         e\n",
       "Ohio    0.517600  0.127705  0.273800\n",
       "Oregon  1.855136  1.452916  0.087474\n",
       "Texas   1.314092  0.307776  1.301489\n",
       "Utah    0.286915  0.756919  1.151341"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frame.sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive Bayes works on Bayes Theorem of probability to predict the class of a given data point. Naive Bayes is extremely fast compared to other classification algorithms and works with an assumption of independence among predictors. \n",
    "\n",
    "The Naive Bayes model is easy to build and particularly useful for very large data sets. Along with simplicity, Naive Bayes is known to outperform even highly sophisticated classification methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge\n",
    "\n",
    "Recall Bayes Theorem, which provides a way of calculating the posterior probability. Its formula is as follows:\n",
    "\n",
    "![alt text](https://github.com/ByteAcademyCo/stats-programmers/blob/master/bayes.png?raw=true \"Logo Title Text 1\")\n",
    "\n",
    "Let's go through an example of how the Naive Bayes Algorithm works using `pandas`. We'll go through a classification problem that determines whether a sports team will play or not based on the weather. \n",
    "\n",
    "First, let's load the module data:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "f1 = pd.read_csv(\"./weather.csv\")\n",
    "print(f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Frequency Table\n",
    "\n",
    "The first actual step of this process is converting the dataset into a frequency table. Using the `groupby()` function, we get the frequencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = f1.groupby(['Weather','Play']).size()\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's split the frequencies by weather and yes/no. Let's start with the three weather frequencies:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = f1.groupby('Weather').count()\n",
    "print(df2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's get the frequencies of yes and no:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = f1.groupby('Play').count()\n",
    "print(df1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Likelihood Table\n",
    "\n",
    "\n",
    "Next, you would create a likelihood table by finding the probabilites of each weather condition and yes/no. This will require that we add a new column that takes the play frequency and divides it by the total data occurances. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['Likelihood'] = df1['Weather']/len(f1)\n",
    "df2['Likelihood'] = df2['Play']/len(f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gets us dataframes that looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df1)\n",
    "print(df2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we're able to use the Naive Bayesian equation to calculate the posterior probability for each class. The highest posterior probability is the outcome of prediction.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculation\n",
    "\n",
    "So now we need a question. Let's propose the following: \"Players will play if the weather is sunny. Is this true?\"\n",
    "\n",
    "From this question, we can construct Bayes Theorem. So what's our P(A|B)? P(Yes|Sunny), which gives us:\n",
    "\n",
    "P(Yes|Sunny) = (P(Sunny|Yes)*P(Yes))/P(Sunny)\n",
    "\n",
    "Based off the likelihood tables we created, we just grab P(Sunny) and P(Yes). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = df2['Likelihood']['Sunny'] # .36\n",
    "print(\"Sunny Likelihood: %f\" % ps) \n",
    "py = df1['Likelihood']['Yes'] # .65\n",
    "print(\"Yes Likelihood: %f\" %py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That leaves us with P(Sunny|Yes). This is the probability that the weather is sunny given that the players played that day. In `df`, we see that the total number of `yes` days under `sunny` is 3. We take this number and divide it by the total number of `yes` days, which we can get from `df`. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "psy = df['Sunny']['Yes']/df1['Weather']['Yes'] # 3/9\n",
    "print(psy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we just have to plug these variables into bayes theorem: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = (psy*py)/ps\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tells us that the answer to our original question is yes!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting Zipfiles\n",
    "\n",
    "Oftentimes, you'll have to download a large number of files. They might come in the form of zipfiles. Instead of manually unzipping them, you can use Python to extract these files for you, using the `os` and `zipfile` modules. \n",
    "\n",
    "### OS Module\n",
    "\n",
    "The `os` module provides us a portable way of using operating system dependent functionality. We'll begin exploring it's capabilities now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the `os.getcwd()` method, we can get the current directory we're in. This is particularly useful when working with a large number of files in a certain folder. In this case, we'll use `os` to work with the zipfiles:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/lesleycordero/Desktop/python-data-prep\n",
      "/Users/lesleycordero/Desktop/python-data-prep/Example\n"
     ]
    }
   ],
   "source": [
    "cwd = os.getcwd()\n",
    "print(cwd)\n",
    "dir_path  = os.path.join(cwd, 'Example')\n",
    "print(dir_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With `os`, you can check to see if a certain directory exists. Here, if it doesn't exist, we create that folder, which we can do with the `os.makedirs()` function:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(dir_path):\n",
    "    os.makedirs(dir_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, as we do with the `ls` command on the terminal, we can use `os.listdir()` to get the contents of whatever path we provide as an argument.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.git',\n",
       " '.gitignore',\n",
       " '.ipynb_checkpoints',\n",
       " 'Data Preparation.ipynb',\n",
       " 'dorms.csv',\n",
       " 'ex.R',\n",
       " 'Example',\n",
       " 'example.zip',\n",
       " 'housing.csv',\n",
       " 'msleep_ggplot2.csv',\n",
       " 'names_add.csv',\n",
       " 'names_extra.csv',\n",
       " 'names_original.csv',\n",
       " 'README.md',\n",
       " 'uk_rain_2014.csv',\n",
       " 'weather.csv']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(cwd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From there, we can move onto working with zipfiles!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ZipFile\n",
    "\n",
    "The `zipfile` module is a powerful module that allows us to extract files from a zipped folder. First, we import the needed modules and assign the name of the zipfile we'll be extracting to a variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import os\n",
    "zip_name = 'example.zip'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we get the current directory path and join it with the zipfile name to get its exact path:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/lesleycordero/Desktop/python-data-prep/example.zip\n"
     ]
    }
   ],
   "source": [
    "cwd = os.getcwd()\n",
    "zip_path = os.path.join(cwd, zip_name)\n",
    "print(zip_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we use the ZipFile class to change the file to a Zipfile object. With this object, we use the `ZipFile.extract()` function to extract all the contents:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "with zipfile.ZipFile(zip_path, 'r') as z:\n",
    "    z.extractall(cwd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And lastly, we can see what's in the zipfile with: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.git',\n",
       " '.gitignore',\n",
       " '.ipynb_checkpoints',\n",
       " 'Data Preparation.ipynb',\n",
       " 'dorms.csv',\n",
       " 'ex.R',\n",
       " 'Example',\n",
       " 'example.zip',\n",
       " 'housing.csv',\n",
       " 'msleep_ggplot2.csv',\n",
       " 'names_add.csv',\n",
       " 'names_extra.csv',\n",
       " 'names_original.csv',\n",
       " 'readings',\n",
       " 'README.md',\n",
       " 'uk_rain_2014.csv',\n",
       " 'weather.csv']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(cwd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Merging\n",
    "\n",
    "If you encounter two different datasets that contain the same type of information, you might consider merging them for your analyses. This is yet another functionality built into `pandas`. \n",
    "\n",
    "Let's go through an example containing student data. `d1` contains 5 of the samples and `d2` contains 2 of them: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  First Name  Last Name\n",
      "0     Lesley    Cordero\n",
      "1       Ojas      Sathe\n",
      "2      Helen       Chen\n",
      "3        Eli   Epperson\n",
      "4      Jacob  Greenberg\n",
      "<class 'pandas.core.frame.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "d1 = pd.read_csv(\"./names_original.csv\")\n",
    "print(d1)\n",
    "print(type(d1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  First Name Last Name\n",
      "0     Martin     Perez\n",
      "1      Menna   Elsayed\n"
     ]
    }
   ],
   "source": [
    "d2 = pd.read_csv(\"./names_add.csv\")\n",
    "print(d2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatenation \n",
    "\n",
    "Instead of working with two separate datasets, it's much easier to simply merge, so we do this with the `concat()` function:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  First Name  Last Name\n",
      "0     Lesley    Cordero\n",
      "1       Ojas      Sathe\n",
      "2      Helen       Chen\n",
      "3        Eli   Epperson\n",
      "4      Jacob  Greenberg\n",
      "0     Martin      Perez\n",
      "1      Menna    Elsayed\n"
     ]
    }
   ],
   "source": [
    "result = pd.concat([d1,d2])\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you might be asking what will happen if one of the datasets has more columns than other - will they still be allowed to merge? Let's try this example with another dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  First Name Last Name                   Major\n",
      "0     Martin     Perez  Mechanical Engineering\n",
      "1      Menna   Elsayed               Sociology\n"
     ]
    }
   ],
   "source": [
    "d3 = pd.read_csv(\"./names_extra.csv\")\n",
    "print(d3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we use the same `concat()` function, we get:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  First Name  Last Name                   Major\n",
      "0     Lesley    Cordero                     NaN\n",
      "1       Ojas      Sathe                     NaN\n",
      "2      Helen       Chen                     NaN\n",
      "3        Eli   Epperson                     NaN\n",
      "4      Jacob  Greenberg                     NaN\n",
      "0     Martin      Perez  Mechanical Engineering\n",
      "1      Menna    Elsayed               Sociology\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:1: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=True'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass sort=False\n",
      "\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "result1 = pd.concat([d1, d3])\n",
    "print(result1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the `NaN` values - these are undefined values indicating there wasn't any data to be displayed. `pandas` will simply fill in the missing data for each sample where it's unavailable:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                       NaN\n",
      "0    Mechanical Engineering\n",
      "Name: Major, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(result1['Major'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging\n",
    "\n",
    "Now, how do we merge two datasets with differing columns? Well, let's take a look at our datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Dorm            Name\n",
      "0  East Campus      Helen Chen\n",
      "1     Broadway   Danielle Jing\n",
      "2      Shapiro    Craig Rhodes\n",
      "3         Watt  Lesley Cordero\n",
      "4  East Campus    Martin Perez\n",
      "5     Broadway   Menna Elsayed\n",
      "6      Wallach   Will Essilfie\n"
     ]
    }
   ],
   "source": [
    "h1 = pd.read_csv(\"./housing.csv\")\n",
    "print(h1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Dorm Street    Cost\n",
      "0     Broadway  114th    9000\n",
      "1      Shapiro  115th    9500\n",
      "2         Watt  113th   10500\n",
      "3  East Campus  116th  11,000\n",
      "4      Wallach  114th    9500\n"
     ]
    }
   ],
   "source": [
    "h2 = pd.read_csv(\"./dorms.csv\")\n",
    "print(h2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the `merge()` function in pandas, we can specify which column to merge on and what kind of join to specify. By default merge does an 'inner' join, but here we set it to a left join:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Dorm            Name Street    Cost\n",
      "0  East Campus      Helen Chen  116th  11,000\n",
      "1     Broadway   Danielle Jing  114th    9000\n",
      "2      Shapiro    Craig Rhodes  115th    9500\n",
      "3         Watt  Lesley Cordero  113th   10500\n",
      "4  East Campus    Martin Perez  116th  11,000\n",
      "5     Broadway   Menna Elsayed  114th    9000\n",
      "6      Wallach   Will Essilfie  114th    9500\n"
     ]
    }
   ],
   "source": [
    "house = pd.merge(h1, h2, on=\"Dorm\", how=\"left\")\n",
    "print(house)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:1: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=True'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass sort=False\n",
      "\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Cost</th>\n",
       "      <th>Dorm</th>\n",
       "      <th>Name</th>\n",
       "      <th>Street</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>East Campus</td>\n",
       "      <td>Helen Chen</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Broadway</td>\n",
       "      <td>Danielle Jing</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Shapiro</td>\n",
       "      <td>Craig Rhodes</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Watt</td>\n",
       "      <td>Lesley Cordero</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>East Campus</td>\n",
       "      <td>Martin Perez</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Broadway</td>\n",
       "      <td>Menna Elsayed</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Wallach</td>\n",
       "      <td>Will Essilfie</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9000</td>\n",
       "      <td>Broadway</td>\n",
       "      <td>NaN</td>\n",
       "      <td>114th</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9500</td>\n",
       "      <td>Shapiro</td>\n",
       "      <td>NaN</td>\n",
       "      <td>115th</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10500</td>\n",
       "      <td>Watt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>113th</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11,000</td>\n",
       "      <td>East Campus</td>\n",
       "      <td>NaN</td>\n",
       "      <td>116th</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9500</td>\n",
       "      <td>Wallach</td>\n",
       "      <td>NaN</td>\n",
       "      <td>114th</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Cost         Dorm            Name Street\n",
       "0     NaN  East Campus      Helen Chen    NaN\n",
       "1     NaN     Broadway   Danielle Jing    NaN\n",
       "2     NaN      Shapiro    Craig Rhodes    NaN\n",
       "3     NaN         Watt  Lesley Cordero    NaN\n",
       "4     NaN  East Campus    Martin Perez    NaN\n",
       "5     NaN     Broadway   Menna Elsayed    NaN\n",
       "6     NaN      Wallach   Will Essilfie    NaN\n",
       "0    9000     Broadway             NaN  114th\n",
       "1    9500      Shapiro             NaN  115th\n",
       "2   10500         Watt             NaN  113th\n",
       "3  11,000  East Campus             NaN  116th\n",
       "4    9500      Wallach             NaN  114th"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.concat([h1,h2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
